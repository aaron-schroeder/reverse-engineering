[
  {
    "objectID": "grade-adjusted-pace.html",
    "href": "grade-adjusted-pace.html",
    "title": "",
    "section": "",
    "text": "import sys\nsys.path.append('../validation')\nfrom common import load_activity_summary_data\nfrom common import load_timeseries_data\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\n\nWhile exploring my personal activity data lake, I found that Strava seemed to be applying the grade adjustment factor to distance, not to pace. That made me wonder if their conceptual model of “running difficulty” was similar to the “cost of running” model used in [[Minetti (2002)]]. In that paper, the “cost” of running was given in terms of energy consumed by the human body, per unit distance, per unit mass. In other words, the units of this quantity are calories per kilogram per kilometer. When choosing a model to assume for my reverse-engineered product, I figured the simpler the better. The embedded assumptions in this model are: - The cost of running depends on grade, but is independent of speed. - Every person’s cost of running curve looks exactly the same. For a given slope, every person spends the same amount of energy (per unit body mass) to cover a mile.\nIf either of these assumptions sounds absurd, remember: the cost of running is in terms of body mass and distance covered. In order to model how much energy a person consumes getting from A to B, the cost of running gets multiplied by the individual’s body mass. And in order to determine the individual’s rate of energy consumption, the quantity is divided by time; to cover A to B in a very short time, a very high burn rate is required.\nHere is the equation:\n\\[C_r(i) = 155.4i^5 - 30.4i^4 -43.3i^3 + 46.3i^2 + 19.5i + 3.6\\]\n\nfrom specialsauce.sources.minetti import cost_of_running\n\ngradient_array = np.linspace(-0.45, 0.45, 100)\nplt.plot(gradient_array, cost_of_running(gradient_array))\nplt.axhline(0.0, color='gray', linewidth=1)\nplt.axvline(0.0, color='gray', linewidth=1)\n\n<matplotlib.lines.Line2D at 0x7efd7bcac550>\n\n\n\n\n\nWhen considering what form to assume for Strava’s grade adjustment factor function, I figured it would be desirable to use the same form as [[Minetti (2002)]]. The general form of that equation is\n\\[\nf(g) = \\Sigma_0^5 c_i  g^i\n\\]\nJust a fifth-order polynomial, where \\(g\\) is decimal grade.\nI have a hunch that Strava’s old Grade Adjusted Pace product was based on using the Minetti equation to compute a ratio of the cost of running on a slope to the cost of running on flat ground:\n\\[\n  f(g) = c_r(g) / c_r(0)\n\\]\n\nadjustment_factor_array = cost_of_running(gradient_array) / cost_of_running(0.0)\n\nplt.axhline(1.0, color='gray', linewidth=1)\nplt.axvline(0.0, color='gray', linewidth=1)\nplt.plot(gradient_array, adjustment_factor_array)\n\n\n\n\n…and I think it would be intuitive to calculate adjusted point-to-point distance like: \\[\n  \\Delta x_g = f(g) \\Delta x\n\\]\nIn words, that means that the adjusted distance from A to B is a function of the grade at point A.\nI want to fit a similar curve to Strava’s “grade adjustment factor” (my term). So how can I make it happen, using the data I have at my fingertips?\n\n\n\n\nI have access to all the time series data for my Strava activities, going back to 2019. In this private dataset, there are over 2.5 million samples, each representing an observation recorded on my activity tracker (a GPS watch). Each observation corresponds to a discrete point in time, and contains a number of variables:\n\npanel_ts_df = load_timeseries_data()\npanel_ts_df.columns\n\nIndex(['lat', 'lon', 'distance', 'altitude', 'velocity_smooth', 'grade_smooth',\n       'cadence', 'heartrate', 'temp', 'watts', 'moving', 'type'],\n      dtype='object')\n\n\nThe variable names largely speak for themselves.\nOnly two of these variables will be used in my analysis. The first, distance, represents cumulative distance traveled from the start of the activity. A bit of rearranging gives me what I am really after: the point-to-point distances between each sample:\n\ndelta_x = panel_ts_df['distance'].groupby('id').diff(1).shift(-1)\ndelta_x.describe()\n\ncount    2.581882e+06\nmean     5.146323e+00\nstd      6.688147e+00\nmin      0.000000e+00\n25%      1.300000e+00\n50%      3.000000e+00\n75%      3.900000e+00\nmax      3.067000e+02\nName: distance, dtype: float64\n\n\nNext, I need to grab a time series that Strava calculates on its backend: grade_smooth. As the name suggests: - This represents grade (terrain slope along the path: \\(\\Delta y / \\Delta x\\)). - The time series has also been smoothed in some way. Since grade represents the change in elevation per change in distance, it is sensitive to noisy data.\nThe grade-smoothing operation is an element of Strava’s special sauce. Until I understand this algorithm (among other secrets), I will be forced to continue uploading my activity data to Strava if I want to see the calculated grade time series. That is irrelevant to this post, but it reflects my long-term ambitions: personal data liberation.\nLet’s get back into the data.\n\ngrade = panel_ts_df['grade_smooth']\ngrade.describe()\n\ncount    2.583119e+06\nmean     1.325603e+00\nstd      1.480232e+01\nmin     -5.000000e+01\n25%     -3.800000e+00\n50%      3.000000e-01\n75%      7.300000e+00\nmax      5.000000e+01\nName: grade_smooth, dtype: float64\n\n\nNote the maximum and minimum values: Strava’s algorithm will never report a grade steeper than 50%. That’s a steepness of 5 meters vertical per 10 meters horizontal. Most terrain does not get that steep, unless you go to the mountains. It looks like I am getting sidetracked again.\n\n\n\nYou know how you can ask for a bulk downloada of all the data you’ve uploaded to apps and services? Strava is no different. And there’s a lot of good stuff in there. What I need is the Grade Adjusted Distance for each activity. Dividing Grade Adjusted Distance by Moving Time yields Grade Adjusted Pace, which is a feature Strava pulled behind their subscription paywall in 2020. Even non-subscribers will see Grade Adjusted Distance in their bulk data download, which really seems like an oversight, but I’ll keep quiet since it facilitates this analysis.\n\nactivities_df = load_activity_summary_data()\ngrade_adjusted_distance = activities_df.set_index('Activity ID')['Grade Adjusted Distance']\ngrade_adjusted_distance.describe()\n\ncount     1163.000000\nmean     12897.031030\nstd       7409.794250\nmin          0.900000\n25%       7235.500000\n50%      12162.000000\n75%      16649.400391\nmax      73662.101562\nName: Grade Adjusted Distance, dtype: float64\n\n\nMeters are the distance units here. So in terms of Grade Adjusted Distance, my longest activity is 73.6 kilometers (and I won’t soon forget it). And my average is somewhere between 12 and 13 kilometers. That’s apparently my sweet spot!\n\n\n\n\n\n\nI could go about this a few different ways. I’m choosing to use sklearn because it facilitates a generalizable approach - one not limited to polynomial regressions.\nNow I’m going to switch hats and start using the domain language of data science. For each activity in my personal data lake, I’ll engineer a number of features I’ll take the time series containing samples of all the variables measured during the activity as well as those calculated by Strava (the activity’s multivariate time series); then I will do some math on them to produce some scalar values - the features. I am extracting features from the multivariate timeseries. In keeping with the approach of [[Minetti (2002)]], the features will be of the form\n\\[\n  ftr_m = \\sum\\limits_{i=0}^{N_{obs}-1} g_i^m \\Delta x(i)\n\\]\nWhy? Maybe let’s take a step back.\nThe total distance of an activity is simply the sum of its point-to-point distance time series.\n\\[\n  x_g = \\sum\\limits_0^{N_{obs}-1} \\Delta x(i)\n\\]\nSimilarly, the Grade Adjusted Distance of an activity is the sum of its adjusted point-to-point distance.\n\\[\n  x_g = \\sum\\limits_0^{N_{obs}-1} f(g(i)) \\Delta x(i)\n\\]\nSince I decided to model the adjustment factor as a polynomial function of grade, this expression becomes\n\\[\n  x_g = \\sum\\limits_{i=0}^{N_{obs}-1} \\sum\\limits_{j=0}^5 c_j \\cdot g^j(i) \\cdot \\Delta x(i)\n\\]\nRemember, all I’m looking to determine in this analysis are the coefficients \\(c_{0..5}\\). Everything else is in the data I have available: each activity’s Grade Adjusted Distance (\\(x_g\\)) as well as all the observations in its time series. To make the data linear-regression-ready, I can engineer features of the form:\n\\[\n  ftr_m = \\sum\\limits_{i=0}^{N_{obs}-1} g^m(i) \\Delta x(i)\n\\]\n…so that for each activity:\n\\[\n  x_g = \\sum\\limits_{j=0}^5 c_j ftr_j\n\\]\n…and when all activities are considered, a linear regression problem appears:\n\\[\n  \\mathbf{x_g} = \\mathbf{F} \\mathbf{c}\n\\]\n…where \\(\\mathbf{F}\\) is the matrix of features that I extract from the activity time series: one row per activity, one column per feature.\nThere is an unwritten rule that basically says: you have to use these specific variables in data science:\n\\[\\mathbf{y} = \\mathbf{X} \\mathbf{b}\\]\nSo to summarize this confusing twist: - \\(\\mathbf{x_g} \\rightarrow \\mathbf{y}\\) (the targets) - \\(\\mathbf{F} \\rightarrow \\mathbf{X}\\) (the features) - \\(\\mathbf{c} \\rightarrow \\mathbf{b}\\) (the coefficients)\nPlease don’t blame me, I am just speaking the vernacular of data science, trying to fit in.\n\npoly_degree = 5\n\nX_raw = pd.concat(\n  [(delta_x * grade ** i).groupby('id').sum() for i in range(poly_degree + 1)],\n  axis=1\n)\n\n\ny_raw = grade_adjusted_distance.dropna()\n\n\nshared_indices = y_raw.index.intersection(X_raw.index)\nX = X_raw.loc[shared_indices, :]\ny = y_raw[shared_indices]\n\nNow to perform the linear regression itself, where I finally get to figure out the coefficients that give me the best fit.\n\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error\n\n\nmodel = LinearRegression()\nmodel.fit(X, y)\npredictions = model.predict(X)\nrmse = mean_squared_error(y, predictions, squared=False)\nrmse\n\n35.85465857601326\n\n\nFor context, the mean and median is on the order of 12000.\nThe fit’s \\(R^2\\) value is close to 1.0. Let’s take a look at a scatterplot of my model’s predicted Grade Adjusted Distance values vs. “observations” from my Strava bulk download:\n\nplt.scatter(y, predictions)\n\n<matplotlib.collections.PathCollection at 0x7efd724665e0>\n\n\n\n\n\nIt looks like a straight line, right on the diagonal. Now, that isn’t a rigorous vetting of the model, but it is quite encouraging. To me, it validates the underlying assumptions - I’m not missing some major detail of Strava’s secret implementation.\n\n\n\n\nIn Strava’s blog post An Improved GAP Model, there is a graph comparing their “improved” grade adjustment factor to their old one based on [[Minetti (2002)]]. I’ll create my own version of this graph, using the two adjustment factor models from my analysis.\n\ngradient_array = np.linspace(-35, 35, 1000)\npoly = np.polynomial.Polynomial(model.coef_)\nfactor_model = poly(gradient_array)\nfactor_minetti = cost_of_running(gradient_array/100.0) / cost_of_running(0.0)\n\nplt.plot(gradient_array, factor_model, color='orange')\nplt.plot(gradient_array, factor_minetti, color='blue')\nplt.axhline(1.0, color='gray', linewidth=1)\nplt.axvline(0.0, color='gray', linewidth=1)\nplt.xlim(-35, 35)\nplt.ylim(0, 4)\nplt.show()\n\n\n\n\nLook familiar??\n\n\n\nadjustment factor comparison plot from Strava blog\n\n\nNow to compare my model of Strava’s adjustment factor to exact values of the adjustment factor I obtained elsewhere (in a story for another time).\n\nfrom specialsauce.datasets import load_ngp_gap\n\ndf_adjusted_pace = load_ngp_gap()\nfactor_experiment = df_adjusted_pace['Pace'] / df_adjusted_pace['GAP']\n\ngradient_array = np.linspace(-50, 50, 100)\nfactor_model = poly(gradient_array)\n\nplt.axhline(1.0, color='gray', linewidth=1)\nplt.axvline(0.0, color='gray', linewidth=1)\nplt.plot(gradient_array, poly(gradient_array))\nplt.scatter(factor_experiment.index, factor_experiment, c='black')\nplt.show()\n\n\n\n\nI’ll take it. The model’s most severe deviation from reality occurs at a grade of 45% - the type of extreme slope that tends to be underrepresented in any runner’s fitness history. It makes sense that the model would underperform in a region of training data scarcity.\n\n\n\nWith such a large number of data points (thanks to 2.5 million discrete samples in my time series history), it was tempting to throw computing power at the problem of reverse-engineering the GAP algorithm. I hope that this post highlights the potential for compute cost savings when a thoughtful modelling approach is adopted."
  }
]